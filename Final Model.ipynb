{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.prelu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        return residual + out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_res_blocks=8, num_channels=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(3, num_channels, kernel_size=9, stride=1, padding=4),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(num_channels) for _ in range(num_res_blocks)])\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_channels)\n",
    "        )\n",
    "        self.upsample_blocks = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, num_channels * 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(num_channels, num_channels * 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.final = nn.Conv2d(num_channels, 3, kernel_size=9, stride=1, padding=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        residual = x\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.middle(x) + residual\n",
    "        x = self.upsample_blocks(x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "class DiscriminatorGlobal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorGlobal, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        features = [64, 128, 256, 512]\n",
    "        layers = []\n",
    "        for i in range(1, len(features)):\n",
    "            layers.append(nn.Conv2d(features[i-1], features[i], kernel_size=3, stride=1, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(features[i]))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append(nn.Conv2d(features[i], features[i], kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(features[i]))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.middle = nn.Sequential(*layers)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(1024, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "class DiscriminatorLocal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorLocal, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.final(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May  7 18:37:51 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.74       Driver Version: 512.74       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   32C    P0    14W /  N/A |      0MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['lr', 'hr'],\n",
      "        num_rows: 800\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['lr', 'hr'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n",
      "lr\n",
      "hr\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "from datasets import load_dataset\n",
    "\n",
    "div2k_dataset = load_dataset(\"eugenesiow/Div2k\")\n",
    "print(div2k_dataset)\n",
    "\n",
    "train_data = div2k_dataset[\"train\"]\n",
    "validation_data = div2k_dataset[\"validation\"]\n",
    "\n",
    "for sample in train_data[:5]:\n",
    "    print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "\n",
    "class Div2kDataset(Dataset):\n",
    "    def __init__(self, div2k_data, transform=None, target_size=(256, 256), patch_size=24):\n",
    "        self.div2k_data = div2k_data\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.div2k_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_image_path = self.div2k_data[idx]['lr']\n",
    "        hr_image_path = self.div2k_data[idx]['hr']\n",
    "        \n",
    "        # Load images\n",
    "        lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "        hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "        \n",
    "        # Resize images to target size\n",
    "        lr_image = lr_image.resize(self.target_size, Image.BICUBIC)\n",
    "        hr_image = hr_image.resize(self.target_size, Image.BICUBIC)\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            lr_image = self.transform(lr_image)\n",
    "            hr_image = self.transform(hr_image)\n",
    "\n",
    "        # Extract patches for local discriminator\n",
    "        patches = self.extract_patches(hr_image)\n",
    "\n",
    "        return lr_image, hr_image, patches\n",
    "\n",
    "    def extract_patches(self, image):\n",
    "        patches = []\n",
    "        _, height, width = image.size()\n",
    "        step = self.patch_size\n",
    "        for i in range(0, height - step + 1, step):\n",
    "            for j in range(0, width - step + 1, step):\n",
    "                patch = image[:, i:i+step, j:j+step]\n",
    "                patches.append(patch)\n",
    "        patches = torch.stack(patches)  # Stack patches into a single tensor\n",
    "        return patches\n",
    "\n",
    "# Load dataset\n",
    "div2k_dataset = load_dataset(\"eugenesiow/Div2k\", split='train[:200]')\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = Div2kDataset(div2k_dataset, transform=transform, target_size=(96, 96), patch_size=24)\n",
    "BATCH_SIZE = 1 # Adjusted batch size for better training dynamics\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR image dimensions: torch.Size([1, 3, 96, 96])\n",
      "HR image dimensions: torch.Size([1, 3, 384, 384])\n",
      "Patch dimensions: torch.Size([256, 3, 24, 24]) Number of patches: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "\n",
    "class Div2kDataset(Dataset):\n",
    "    def __init__(self, div2k_data, lr_transform=None, hr_transform=None, patch_size=24):\n",
    "        self.div2k_data = div2k_data\n",
    "        self.lr_transform = lr_transform\n",
    "        self.hr_transform = hr_transform\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.div2k_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_image_path = self.div2k_data[idx]['lr']\n",
    "        hr_image_path = self.div2k_data[idx]['hr']\n",
    "\n",
    "        lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "        hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "\n",
    "        if self.lr_transform:\n",
    "            lr_image = self.lr_transform(lr_image)\n",
    "        if self.hr_transform:\n",
    "            hr_image = self.hr_transform(hr_image)\n",
    "\n",
    "        patches = self.extract_patches(hr_image)\n",
    "\n",
    "        return lr_image, hr_image, patches\n",
    "\n",
    "    def extract_patches(self, image):\n",
    "        patches = []\n",
    "        c, height, width = image.shape\n",
    "        step = self.patch_size\n",
    "        for i in range(0, height - step + 1, step):\n",
    "            for j in range(0, width - step + 1, step):\n",
    "                patch = image[:, i:i+step, j:j+step]\n",
    "                patches.append(patch)\n",
    "        return torch.stack(patches)\n",
    "\n",
    "# Load dataset\n",
    "div2k_dataset = load_dataset(\"eugenesiow/Div2k\", split='train[:200]')\n",
    "\n",
    "# Define transformations\n",
    "lr_transform = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "hr_transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset instance with transformations\n",
    "train_dataset = Div2kDataset(div2k_dataset, lr_transform=lr_transform, hr_transform=hr_transform, patch_size=24)\n",
    "BATCH_SIZE = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Checking dimensions\n",
    "lr_example, hr_example, patches = next(iter(train_loader))\n",
    "print(\"LR image dimensions:\", lr_example.shape)\n",
    "print(\"HR image dimensions:\", hr_example.shape)\n",
    "print(\"Patch dimensions:\", patches.shape[1:], \"Number of patches:\", patches.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGGFeatureExtractor(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self, feature_layer=34, use_bn=False):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        # Load pretrained VGG19 model\n",
    "        vgg = models.vgg19(pretrained=True)\n",
    "        # Use features up to the selected layer to act as our feature extractor\n",
    "        self.feature_extractor = nn.Sequential(*list(vgg.features.children())[:feature_layer + 1])\n",
    "        self.use_bn = use_bn\n",
    "        # If batch normalization is used, initialize mean and std\n",
    "        if use_bn:\n",
    "            self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "            self.std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Normalize the image batch using the same normalization used in training VGG if batch normalization is enabled\n",
    "        if self.use_bn:\n",
    "            x = (x - self.mean.to(x.device)) / self.std.to(x.device)\n",
    "        # Extract features\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "# Ensure that all components are sent to the same device as the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_extractor = VGGFeatureExtractor(feature_layer=34, use_bn=True).to(device)\n",
    "feature_extractor.eval()  # Set to eval mode to freeze batch norm layers and dropout layers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Batch [1/200], Generator Loss: 0.5371, Discriminator Global Loss: 0.7200, Discriminator Local Loss: 1.4484\n",
      "Epoch [1/100], Batch [101/200], Generator Loss: 0.1205, Discriminator Global Loss: 0.5893, Discriminator Local Loss: 1.3082\n",
      "Epoch [2/100], Batch [1/200], Generator Loss: 0.1756, Discriminator Global Loss: 0.5034, Discriminator Local Loss: 1.2778\n",
      "Epoch [2/100], Batch [101/200], Generator Loss: 0.1471, Discriminator Global Loss: 0.5034, Discriminator Local Loss: 1.2119\n",
      "Epoch [3/100], Batch [1/200], Generator Loss: 0.0620, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.1855\n",
      "Epoch [3/100], Batch [101/200], Generator Loss: 0.0824, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.1514\n",
      "Epoch [4/100], Batch [1/200], Generator Loss: 0.0926, Discriminator Global Loss: 0.6410, Discriminator Local Loss: 1.0683\n",
      "Epoch [4/100], Batch [101/200], Generator Loss: 0.0638, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0825\n",
      "Epoch [5/100], Batch [1/200], Generator Loss: 0.1788, Discriminator Global Loss: 0.5033, Discriminator Local Loss: 1.1154\n",
      "Epoch [5/100], Batch [101/200], Generator Loss: 0.0950, Discriminator Global Loss: 0.5033, Discriminator Local Loss: 1.0471\n",
      "Epoch [6/100], Batch [1/200], Generator Loss: 0.0532, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0388\n",
      "Epoch [6/100], Batch [101/200], Generator Loss: 0.0602, Discriminator Global Loss: 0.5050, Discriminator Local Loss: 1.0202\n",
      "Epoch [7/100], Batch [1/200], Generator Loss: 0.0622, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0248\n",
      "Epoch [7/100], Batch [101/200], Generator Loss: 0.0617, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0242\n",
      "Epoch [8/100], Batch [1/200], Generator Loss: 0.0261, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0667\n",
      "Epoch [8/100], Batch [101/200], Generator Loss: 0.0437, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0246\n",
      "Epoch [9/100], Batch [1/200], Generator Loss: 0.0653, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0130\n",
      "Epoch [9/100], Batch [101/200], Generator Loss: 0.0317, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0471\n",
      "Epoch [10/100], Batch [1/200], Generator Loss: 0.0519, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0432\n",
      "Epoch [10/100], Batch [101/200], Generator Loss: 0.0325, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0258\n",
      "Epoch [11/100], Batch [1/200], Generator Loss: 0.0763, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0090\n",
      "Epoch [11/100], Batch [101/200], Generator Loss: 0.0516, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0545\n",
      "Epoch [12/100], Batch [1/200], Generator Loss: 0.0674, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0754\n",
      "Epoch [12/100], Batch [101/200], Generator Loss: 0.0382, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0443\n",
      "Epoch [13/100], Batch [1/200], Generator Loss: 0.0645, Discriminator Global Loss: 0.5032, Discriminator Local Loss: 1.0126\n",
      "Epoch [13/100], Batch [101/200], Generator Loss: 0.0322, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0208\n",
      "Epoch [14/100], Batch [1/200], Generator Loss: 0.0464, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0230\n",
      "Epoch [14/100], Batch [101/200], Generator Loss: 0.0399, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0154\n",
      "Epoch [15/100], Batch [1/200], Generator Loss: 0.0728, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0171\n",
      "Epoch [15/100], Batch [101/200], Generator Loss: 0.0376, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0599\n",
      "Epoch [16/100], Batch [1/200], Generator Loss: 0.0319, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0300\n",
      "Epoch [16/100], Batch [101/200], Generator Loss: 0.0404, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0705\n",
      "Epoch [17/100], Batch [1/200], Generator Loss: 0.0458, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0170\n",
      "Epoch [17/100], Batch [101/200], Generator Loss: 0.0614, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0188\n",
      "Epoch [18/100], Batch [1/200], Generator Loss: 0.0371, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0102\n",
      "Epoch [18/100], Batch [101/200], Generator Loss: 0.0492, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0083\n",
      "Epoch [19/100], Batch [1/200], Generator Loss: 0.0395, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0420\n",
      "Epoch [19/100], Batch [101/200], Generator Loss: 0.0803, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0115\n",
      "Epoch [20/100], Batch [1/200], Generator Loss: 0.1910, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0069\n",
      "Epoch [20/100], Batch [101/200], Generator Loss: 0.1053, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0098\n",
      "Epoch [21/100], Batch [1/200], Generator Loss: 0.0445, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0285\n",
      "Epoch [21/100], Batch [101/200], Generator Loss: 0.0306, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0125\n",
      "Epoch [22/100], Batch [1/200], Generator Loss: 0.0313, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0131\n",
      "Epoch [22/100], Batch [101/200], Generator Loss: 0.0290, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0171\n",
      "Epoch [23/100], Batch [1/200], Generator Loss: 0.0494, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0306\n",
      "Epoch [23/100], Batch [101/200], Generator Loss: 0.0419, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0087\n",
      "Epoch [24/100], Batch [1/200], Generator Loss: 0.0372, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0410\n",
      "Epoch [24/100], Batch [101/200], Generator Loss: 0.0489, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0153\n",
      "Epoch [25/100], Batch [1/200], Generator Loss: 0.0723, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0090\n",
      "Epoch [25/100], Batch [101/200], Generator Loss: 0.0560, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0325\n",
      "Epoch [26/100], Batch [1/200], Generator Loss: 0.0388, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0149\n",
      "Epoch [26/100], Batch [101/200], Generator Loss: 0.0337, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0430\n",
      "Epoch [27/100], Batch [1/200], Generator Loss: 0.0489, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0134\n",
      "Epoch [27/100], Batch [101/200], Generator Loss: 0.0661, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0103\n",
      "Epoch [28/100], Batch [1/200], Generator Loss: 0.0291, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0631\n",
      "Epoch [28/100], Batch [101/200], Generator Loss: 0.0226, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0563\n",
      "Epoch [29/100], Batch [1/200], Generator Loss: 0.0731, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0076\n",
      "Epoch [29/100], Batch [101/200], Generator Loss: 0.0429, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0224\n",
      "Epoch [30/100], Batch [1/200], Generator Loss: 0.0471, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0086\n",
      "Epoch [30/100], Batch [101/200], Generator Loss: 0.0281, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.1314\n",
      "Epoch [31/100], Batch [1/200], Generator Loss: 0.0597, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0178\n",
      "Epoch [31/100], Batch [101/200], Generator Loss: 0.0368, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0100\n",
      "Epoch [32/100], Batch [1/200], Generator Loss: 0.0274, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0346\n",
      "Epoch [32/100], Batch [101/200], Generator Loss: 0.0398, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0156\n",
      "Epoch [33/100], Batch [1/200], Generator Loss: 0.0308, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0258\n",
      "Epoch [33/100], Batch [101/200], Generator Loss: 0.0276, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0172\n",
      "Epoch [34/100], Batch [1/200], Generator Loss: 0.0150, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0288\n",
      "Epoch [34/100], Batch [101/200], Generator Loss: 0.0452, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0099\n",
      "Epoch [35/100], Batch [1/200], Generator Loss: 0.0738, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0080\n",
      "Epoch [35/100], Batch [101/200], Generator Loss: 0.0478, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0181\n",
      "Epoch [36/100], Batch [1/200], Generator Loss: 0.0379, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0092\n",
      "Epoch [36/100], Batch [101/200], Generator Loss: 0.0374, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0296\n",
      "Epoch [37/100], Batch [1/200], Generator Loss: 0.0383, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0096\n",
      "Epoch [37/100], Batch [101/200], Generator Loss: 0.0215, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0097\n",
      "Epoch [38/100], Batch [1/200], Generator Loss: 0.0560, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0077\n",
      "Epoch [38/100], Batch [101/200], Generator Loss: 0.0894, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0152\n",
      "Epoch [39/100], Batch [1/200], Generator Loss: 0.0394, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0089\n",
      "Epoch [39/100], Batch [101/200], Generator Loss: 0.0350, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0113\n",
      "Epoch [40/100], Batch [1/200], Generator Loss: 0.0334, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0152\n",
      "Epoch [40/100], Batch [101/200], Generator Loss: 0.0716, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0071\n",
      "Epoch [41/100], Batch [1/200], Generator Loss: 0.0612, Discriminator Global Loss: 0.6931, Discriminator Local Loss: 1.0230\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator_global = DiscriminatorGlobal().to(device)\n",
    "discriminator_local = DiscriminatorLocal().to(device)\n",
    "feature_extractor = VGGFeatureExtractor(feature_layer=34, use_bn=False).to(device)\n",
    "feature_extractor.eval()  # Ensure the feature extractor is not in training mode\n",
    "\n",
    "# Define loss functions\n",
    "criterion_bce = nn.BCEWithLogitsLoss()\n",
    "criterion_pixel = nn.L1Loss()\n",
    "criterion_feature = nn.MSELoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_global = optim.Adam(discriminator_global.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_local = optim.Adam(discriminator_local.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "adversarial_loss_weight = 1e-3\n",
    "pixel_loss_weight = 1\n",
    "feature_loss_weight = 1e-3\n",
    "log_interval = 100\n",
    "save_interval = 2\n",
    "checkpoint_path = r'C:\\Users\\juver\\Untitled Folder 1\\model_checkpoint3.pth'\n",
    "\n",
    "def extract_patches(images, patch_size=24):\n",
    "    patches = []\n",
    "    batch_size, channels, height, width = images.size()\n",
    "    for i in range(0, height - patch_size + 1, patch_size):\n",
    "        for j in range(0, width - patch_size + 1, patch_size):\n",
    "            patches.append(images[:, :, i:i+patch_size, j:j+patch_size])\n",
    "    return torch.stack(patches, dim=0)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator_global.train()\n",
    "    discriminator_local.train()\n",
    "\n",
    "    for batch_idx, (lr_images, hr_images, _) in enumerate(train_loader):\n",
    "        lr_images, hr_images = lr_images.to(device), hr_images.to(device)\n",
    "        \n",
    "        # Generate high-resolution images from low-resolution images\n",
    "        fake_hr_images = generator(lr_images)\n",
    "\n",
    "        # Global Discriminator Update\n",
    "        optimizer_D_global.zero_grad()\n",
    "        real_output_global = discriminator_global(hr_images)\n",
    "        fake_output_global = discriminator_global(fake_hr_images.detach())\n",
    "        loss_D_global = (criterion_bce(real_output_global, torch.ones_like(real_output_global)) +\n",
    "                         criterion_bce(fake_output_global, torch.zeros_like(fake_output_global))) / 2\n",
    "        loss_D_global.backward()\n",
    "        optimizer_D_global.step()\n",
    "\n",
    "        # Local Discriminator Update\n",
    "        hr_patches = extract_patches(hr_images)\n",
    "        fake_hr_patches = extract_patches(fake_hr_images.detach())\n",
    "        optimizer_D_local.zero_grad()\n",
    "        loss_D_local = 0\n",
    "        num_patches = hr_patches.shape[0]\n",
    "        for i in range(num_patches):\n",
    "            real_output_local = discriminator_local(hr_patches[i])\n",
    "            fake_output_local = discriminator_local(fake_hr_patches[i])\n",
    "            loss_D_local += (criterion_bce(real_output_local, torch.ones_like(real_output_local)) +\n",
    "                             criterion_bce(fake_output_local, torch.zeros_like(fake_output_local))) / num_patches\n",
    "        loss_D_local.backward()\n",
    "        optimizer_D_local.step()\n",
    "\n",
    "        # Generator Update\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_output_global = discriminator_global(fake_hr_images)\n",
    "        adversarial_loss = criterion_bce(fake_output_global, torch.ones_like(fake_output_global))\n",
    "        pixel_loss = criterion_pixel(fake_hr_images, hr_images)\n",
    "\n",
    "        # Compute feature loss using VGG\n",
    "        real_features = feature_extractor(hr_images)\n",
    "        fake_features = feature_extractor(fake_hr_images)\n",
    "        feature_loss = criterion_feature(fake_features, real_features)\n",
    "\n",
    "        total_loss = adversarial_loss_weight * adversarial_loss + pixel_loss_weight * pixel_loss + feature_loss_weight * feature_loss\n",
    "        total_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                  f\"Generator Loss: {total_loss.item():.4f}, Discriminator Global Loss: {loss_D_global.item():.4f}, \"\n",
    "                  f\"Discriminator Local Loss: {loss_D_local.item():.4f}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_global_state_dict': discriminator_global.state_dict(),\n",
    "            'discriminator_local_state_dict': discriminator_local.state_dict(),\n",
    "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "            'optimizer_D_global_state_dict': optimizer_D_global.state_dict(),\n",
    "            'optimizer_D_local_state_dict': optimizer_D_local.state_dict(),\n",
    "            'total_loss': total_loss\n",
    "        }, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
